{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f49b3c",
   "metadata": {},
   "source": [
    "# 4. Truncation without training on 1000 samples t5-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219af75",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939b01c",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef95dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0aa9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/test_set.txt') as json_file:\n",
    "    test_set = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34570c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = len(test_set['document'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6659e9b",
   "metadata": {},
   "source": [
    "## 2. Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b953373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, actuals, tokenizer):\n",
    "    \n",
    "    # <your code here>\n",
    "    metric = load_metric(\"rouge\")\n",
    "    result = metric.compute(predictions=predictions, references=actuals, use_stemmer=True)\n",
    "    \n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    rouge = {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "    return rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44abbe",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a144c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gniemiec/t5-small-finetuned-xsum\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL = \"gniemiec/t5-small-finetuned-xsum\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL)\n",
    "\n",
    "print(f\"Loaded {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d33621",
   "metadata": {},
   "source": [
    "## 4. Evaluation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3370d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "device = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "534ddc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", all 18, and a woman who has been charged with hate crimes, have appeared in court on Friday.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # tokens = tokenizer(test_set['document'][1], return_tensors=\"pt\")\n",
    "    tokens = tokenizer.batch_encode_plus([test_set['document'][1]], max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    # outputs = model.generate(tokens)\n",
    "    outputs = model.to(device).generate(\n",
    "                      input_ids = tokens.input_ids,\n",
    "                      attention_mask = tokens.attention_mask, \n",
    "                      max_length=150, \n",
    "                      num_beams=2,\n",
    "                      repetition_penalty=2.5, \n",
    "                      length_penalty=1.0, \n",
    "                      early_stopping=True\n",
    "                      )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "428f0626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge1</th>\n",
       "      <td>34.2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2</th>\n",
       "      <td>12.1212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL</th>\n",
       "      <td>28.5714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum</th>\n",
       "      <td>28.5714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gen_len</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "rouge1     34.2857\n",
       "rouge2     12.1212\n",
       "rougeL     28.5714\n",
       "rougeLsum  28.5714\n",
       "gen_len     1.0000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = compute_metrics([decoded], [test_set['summary'][0]], tokenizer)   \n",
    "rouge_df = pd.DataFrame.from_dict(rouge, orient='index')\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a471d52",
   "metadata": {},
   "source": [
    "## 5. Evaluate on 1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42def54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Epoch 0 | 1000\n",
      "Completed Epoch 200 | 1000\n",
      "Completed Epoch 400 | 1000\n",
      "Completed Epoch 600 | 1000\n",
      "Completed Epoch 800 | 1000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model = model.to(device)\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, doc in enumerate(test_set['document']):\n",
    "        if idx % 200 == 0:\n",
    "            print(f\"Completed Epoch {idx} | {data_length}\")\n",
    "        tokens = tokenizer.batch_encode_plus([doc], \n",
    "                                             max_length=max_length, \n",
    "                                             truncation=True, \n",
    "                                             padding=\"max_length\", \n",
    "                                             return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(\n",
    "                      input_ids = tokens.input_ids,\n",
    "                      attention_mask = tokens.attention_mask, \n",
    "                      max_length=150, \n",
    "                      num_beams=2,\n",
    "                      repetition_penalty=2.5, \n",
    "                      length_penalty=1.0, \n",
    "                      early_stopping=True\n",
    "                      )\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22faa3",
   "metadata": {},
   "source": [
    "### 5.1 Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20339dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(predictions, actuals):\n",
    "    df = pd.DataFrame({'predictions': predictions, 'actuals': actuals})\n",
    "    df.to_csv('outputs/predictions_pretrained_t5xsum.csv')\n",
    "    print(\"PREDICTIONS RESULTS SAVED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1e64865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    }
   ],
   "source": [
    "save_results(predictions, test_set['summary'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
