{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f49b3c",
   "metadata": {},
   "source": [
    "# Evaluation on truncated tokens t5-pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219af75",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c0215b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  9% |\n",
      "|  1 |  0% | 77% |\n",
      "|  2 |  0% | 11% |\n",
      "|  3 |  0% | 98% |\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939b01c",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef95dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "\n",
    "from bert_score import score\n",
    "from bert_score import plot_example\n",
    "\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, tokenization_utils_base, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "device = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0aa9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "with open('../../datasets/test_set.txt') as json_file:\n",
    "    test_set = json.load(json_file)\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb95ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P, R, F1 = score([test_set['document'][0], test_set['document'][1]], [test_set['summary'][0], test_set['summary'][1]], lang='en', rescale_with_baseline=True, device=device)\n",
    "# # P, R, F1 = scorer.score([test_set['document'][0]], [test_set['summary'][0]])\n",
    "# # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "# print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6659e9b",
   "metadata": {},
   "source": [
    "## 2. Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b953373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, actuals, tokenizer):\n",
    "    \n",
    "    # <your code here>\n",
    "    metric = load_metric(\"rouge\")\n",
    "    result = metric.compute(predictions=predictions, references=actuals, use_stemmer=True)\n",
    "    \n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    rouge = {k: round(v, 4) for k, v in result.items()}\n",
    "    \n",
    "    P, R, F1 = score(predictions, actuals, lang='en', rescale_with_baseline=True, device=device)\n",
    "    \n",
    "    bert = {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'F1': F1.mean().item(),\n",
    "    }\n",
    "\n",
    "    return rouge, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44abbe",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd42387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define model parameters specific to BART\n",
    "model_params = {\n",
    "    \"MODEL\": \"gniemiec/t5-small-finetuned-xsum\",\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 36,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a144c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gniemiec/t5-small-finetuned-xsum\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "print(f\"Loaded {model_params['MODEL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d33621",
   "metadata": {},
   "source": [
    "## 4. Test 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "534ddc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Police have recovered three firearms, ammunition and a five-figure sum of money.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # tokens = tokenizer(test_set['document'][1], return_tensors=\"pt\")\n",
    "    tokens = tokenizer.batch_encode_plus([test_set['document'][0]], \n",
    "                                         max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"], \n",
    "                                         truncation=True, \n",
    "                                         padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    # outputs = model.generate(tokens)\n",
    "    outputs = model.to(device).generate(\n",
    "                      input_ids = tokens.input_ids,\n",
    "                      attention_mask = tokens.attention_mask, \n",
    "                      max_length=150, \n",
    "                      num_beams=2,\n",
    "                      repetition_penalty=2.5, \n",
    "                      length_penalty=1.0, \n",
    "                      early_stopping=True\n",
    "                      )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428f0626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge1</th>\n",
       "      <td>33.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2</th>\n",
       "      <td>14.2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL</th>\n",
       "      <td>20.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum</th>\n",
       "      <td>20.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gen_len</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "rouge1     33.3333\n",
       "rouge2     14.2857\n",
       "rougeL     20.0000\n",
       "rougeLsum  20.0000\n",
       "gen_len     1.0000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge, bert = compute_metrics([decoded], [test_set['summary'][0]], tokenizer)   \n",
    "rouge_df = pd.DataFrame.from_dict(rouge, orient='index')\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aadac57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.499494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.473107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.487103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "precision  0.499494\n",
       "recall     0.473107\n",
       "F1         0.487103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df = pd.DataFrame.from_dict(bert, orient='index')\n",
    "bert_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a471d52",
   "metadata": {},
   "source": [
    "## 5. Evaluate on 1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7c97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix} | {iteration}/{total}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b42def54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# model = model.to(device)\n",
    "# predictions = []\n",
    "\n",
    "# l = len(test_set['document'])\n",
    "# printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "# with torch.no_grad():\n",
    "#     for idx, doc in enumerate(test_set['document']):\n",
    "#         printProgressBar(idx + 1, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "#         tokens = tokenizer.batch_encode_plus([doc], \n",
    "#                                              max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"], \n",
    "#                                              truncation=True, \n",
    "#                                              padding=\"max_length\", \n",
    "#                                              return_tensors=\"pt\").to(device)\n",
    "       \n",
    "#         outputs = model.generate(\n",
    "#                       input_ids = tokens.input_ids,\n",
    "#                       attention_mask = tokens.attention_mask, \n",
    "#                       max_length=150, \n",
    "#                       num_beams=2,\n",
    "#                       repetition_penalty=2.5, \n",
    "#                       length_penalty=1.0, \n",
    "#                       early_stopping=True\n",
    "#                       )\n",
    "#         decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#         predictions.append(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22faa3",
   "metadata": {},
   "source": [
    "### 5.1 Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20339dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(predictions, actuals, output):\n",
    "    df = pd.DataFrame({'predictions': predictions, 'actuals': actuals})\n",
    "    df.to_csv(output)\n",
    "    print(\"PREDICTIONS RESULTS SAVED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e64865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_results(predictions, test_set['summary'], 'outputs/predictions_pretrained_t5xsum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c133c37",
   "metadata": {},
   "source": [
    "## 6. Create truncated tokens list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7011cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Eval(output_dir, strategy=\"none\", no_tokens=0, from_left = 0.2):\n",
    "    '''\n",
    "    source: array of documents\n",
    "    strategy:\n",
    "        - head: truncate head\n",
    "        - tail: truncate tail \n",
    "        - both: truncate head and tail\n",
    "        - middle: truncate middle words\n",
    "    words_no: number of tokens to remove\n",
    "    '''\n",
    "    print(f\"Strategy: Remove {strategy} tokens\")\n",
    "    l = len(test_set['document'])\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seedtokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    \n",
    "    print(\"Loading model..\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_params[\"MODEL\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, doc in enumerate(test_set['document']):\n",
    "            printProgressBar(idx + 1, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "            tokens = tokenizer.batch_encode_plus([doc], \n",
    "                                                 max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"], \n",
    "                                                 truncation=True, \n",
    "                                                 padding=\"max_length\", \n",
    "                                                 return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            source = tokens.input_ids[0]\n",
    "            attention = tokens.attention_mask[0]\n",
    "            \n",
    "            if strategy == \"middle\":\n",
    "                \n",
    "                left_remove_percent = from_left\n",
    "                \n",
    "                no_real_tokens = sum(x != 0 for x in source)\n",
    "                left = int(left_remove_percent * no_real_tokens)\n",
    "                \n",
    "                # 3. Truncate left and right, concat both (input ids and attention mask)\n",
    "                left_selection_ids = source[:left]\n",
    "                right_selection_ids = source[left+no_tokens:len(source)]\n",
    "\n",
    "                left_attention_mask = attention[:left]\n",
    "                right_attention_mask = attention[left+no_tokens:len(source)]\n",
    "\n",
    "                new_ids = torch.concat([left_selection_ids, right_selection_ids], 0)        \n",
    "                new_masks = torch.concat([left_attention_mask,right_attention_mask], 0)\n",
    "\n",
    "                new_ids = new_ids.tolist()\n",
    "                new_masks = new_masks.tolist()\n",
    "\n",
    "                # 4. Create a new source text\n",
    "                new_encoding = {\n",
    "                    'input_ids': torch.IntTensor([new_ids]).to(dtype=torch.long),\n",
    "                    'attention_mask': torch.IntTensor([new_masks]).to(dtype=torch.long)\n",
    "                }\n",
    "                \n",
    "\n",
    "                # 5. Set new token encoding\n",
    "                new_tokens = tokenization_utils_base.BatchEncoding(new_encoding).to(device)\n",
    "                \n",
    "                \n",
    "            elif strategy == \"head\":\n",
    "                new_ids = source[no_tokens:].tolist()\n",
    "                new_masks = attention[no_tokens:].tolist()\n",
    "\n",
    "                # 4. Create a new source text\n",
    "                new_encoding = {\n",
    "                    'input_ids': torch.IntTensor([new_ids]).to(dtype=torch.long),\n",
    "                    'attention_mask': torch.IntTensor([new_masks]).to(dtype=torch.long)\n",
    "                }\n",
    "                \n",
    "\n",
    "                # 5. Set new token encoding\n",
    "                new_tokens = tokenization_utils_base.BatchEncoding(new_encoding).to(device)\n",
    "                \n",
    "            elif strategy == \"tail\":\n",
    "                no_real_tokens = sum(x != 0 for x in source)\n",
    "                zeros = source[no_real_tokens:]\n",
    "                \n",
    "                new_ids = torch.concat([source[:no_real_tokens-no_tokens],zeros], 0).tolist()\n",
    "                new_masks = torch.concat([attention[:no_real_tokens-no_tokens],zeros], 0).tolist()\n",
    "                \n",
    "                \n",
    "                new_encoding = {\n",
    "                    'input_ids': torch.IntTensor([new_ids]).to(dtype=torch.long),\n",
    "                    'attention_mask': torch.IntTensor([new_masks]).to(dtype=torch.long)\n",
    "                }\n",
    "                new_tokens = tokenization_utils_base.BatchEncoding(new_encoding).to(device)\n",
    "                \n",
    "            elif strategy == \"both\":\n",
    "                pass\n",
    "            else:\n",
    "                new_tokens = tokens\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                          input_ids = new_tokens.input_ids,\n",
    "                          attention_mask = new_tokens.attention_mask, \n",
    "                          max_length=150, \n",
    "                          num_beams=2,\n",
    "                          repetition_penalty=2.5, \n",
    "                          length_penalty=1.0, \n",
    "                          early_stopping=True\n",
    "                          )\n",
    "            \n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predictions.append(decoded)\n",
    "            \n",
    "    save_results(predictions, test_set['summary'], os.path.join(output_dir,f'predictions_{strategy}_{no_tokens}.csv'))\n",
    "    \n",
    "    rouge, bert = compute_metrics(predictions, test_set['summary'], tokenizer)\n",
    "    \n",
    "    rouge_df = pd.DataFrame.from_dict(rouge, orient='index')\n",
    "    rouge_df.to_csv(os.path.join(output_dir, f'rouge_score_{strategy}_{no_tokens}.csv'))\n",
    "    \n",
    "    bert_df = pd.DataFrame.from_dict(bert, orient='index')\n",
    "    bert_df.to_csv(os.path.join(output_dir, f'bert_score_{strategy}_{no_tokens}.csv'))\n",
    "    \n",
    "    print(f\"SAVE ROUGE TO CSV FINISHED @ {os.path.join(output_dir, f'rouge_score_{strategy}_{no_tokens}.csv')}\")\n",
    "    print(f\"SAVE BERT-SCORE TO CSV FINISHED @ {os.path.join(output_dir, f'bert_score_{strategy}_{no_tokens}.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3b5e3",
   "metadata": {},
   "source": [
    "## 7. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb8b18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_remove = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac61102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove middle tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_middle_20.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_middle_20.csv\n"
     ]
    }
   ],
   "source": [
    "# #middle words\n",
    "Eval(output_dir='outputs/', strategy=\"middle\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58d0d214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>21.7266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>15.9697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>15.9454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0        0\n",
       "0     rouge1  21.7266\n",
       "1     rouge2   4.1710\n",
       "2     rougeL  15.9697\n",
       "3  rougeLsum  15.9454\n",
       "4    gen_len   1.0000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df_middle = pd.read_csv(f'outputs/rouge_score_middle_{tokens_to_remove}.csv')\n",
    "rouge_df_middle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdc781d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove none tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_none_0.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_none_0.csv\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "Eval(output_dir='outputs/', strategy=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91d01335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>22.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.4585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.3347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.3285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0        0\n",
       "0     rouge1  22.2732\n",
       "1     rouge2   4.4585\n",
       "2     rougeL  16.3347\n",
       "3  rougeLsum  16.3285\n",
       "4    gen_len   1.0000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df = pd.read_csv('outputs/rouge_score_none_0.csv')\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59eb547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove head tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_head_20.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_head_20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "#head words\n",
    "Eval(output_dir='outputs/', strategy=\"head\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88756085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>21.8455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.3569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0        0\n",
       "0     rouge1  21.8455\n",
       "1     rouge2   4.3569\n",
       "2     rougeL  16.1102\n",
       "3  rougeLsum  16.1011\n",
       "4    gen_len   1.0000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df_head = pd.read_csv(f'outputs/rouge_score_head_{tokens_to_remove}.csv')\n",
    "rouge_df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7430686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove tail tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_tail_20.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_tail_20.csv\n"
     ]
    }
   ],
   "source": [
    "#tail words\n",
    "Eval(output_dir='outputs/', strategy=\"tail\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "753d07a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>20.2834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>3.2438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>14.1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>14.1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0        0\n",
       "0     rouge1  20.2834\n",
       "1     rouge2   3.2438\n",
       "2     rougeL  14.1731\n",
       "3  rougeLsum  14.1632\n",
       "4    gen_len   1.0000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df_tail = pd.read_csv(f'outputs/rouge_score_tail_{tokens_to_remove}.csv')\n",
    "rouge_df_tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a2866",
   "metadata": {},
   "source": [
    "## Trying with 50 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dbcb149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove middle tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_middle_50.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_middle_50.csv\n",
      "Strategy: Remove head tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_head_50.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_head_50.csv\n",
      "Strategy: Remove tail tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_tail_50.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_tail_50.csv\n"
     ]
    }
   ],
   "source": [
    "tokens_to_remove = 50\n",
    "Eval(output_dir='outputs/', strategy=\"middle\", no_tokens=tokens_to_remove)\n",
    "Eval(output_dir='outputs/', strategy=\"head\", no_tokens=tokens_to_remove)\n",
    "Eval(output_dir='outputs/', strategy=\"tail\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34d54ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>none</th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "      <th>middle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>22.2732</td>\n",
       "      <td>20.4727</td>\n",
       "      <td>19.8081</td>\n",
       "      <td>21.0320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.4585</td>\n",
       "      <td>3.7730</td>\n",
       "      <td>2.9725</td>\n",
       "      <td>3.8092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.3347</td>\n",
       "      <td>15.0436</td>\n",
       "      <td>13.8637</td>\n",
       "      <td>15.5697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.3285</td>\n",
       "      <td>15.0360</td>\n",
       "      <td>13.8608</td>\n",
       "      <td>15.5187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0     none     head     tail   middle\n",
       "0     rouge1  22.2732  20.4727  19.8081  21.0320\n",
       "1     rouge2   4.4585   3.7730   2.9725   3.8092\n",
       "2     rougeL  16.3347  15.0436  13.8637  15.5697\n",
       "3  rougeLsum  16.3285  15.0360  13.8608  15.5187\n",
       "4    gen_len   1.0000   1.0000   1.0000   1.0000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df_50 = pd.read_csv('outputs/rouge_score_none_0.csv').rename(columns={'0': 'none'})\n",
    "comparison_df_50['head'] = pd.read_csv(f'outputs/rouge_score_head_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_50['tail']= pd.read_csv(f'outputs/rouge_score_tail_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_50['middle'] = pd.read_csv(f'outputs/rouge_score_middle_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44827817",
   "metadata": {},
   "source": [
    "## Trying with 10 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f712dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove middle tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_middle_10.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_middle_10.csv\n",
      "Strategy: Remove head tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_head_10.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_head_10.csv\n",
      "Strategy: Remove tail tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_tail_10.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_tail_10.csv\n"
     ]
    }
   ],
   "source": [
    "tokens_to_remove = 10\n",
    "Eval(output_dir='outputs/', strategy=\"middle\", no_tokens=tokens_to_remove)\n",
    "Eval(output_dir='outputs/', strategy=\"head\", no_tokens=tokens_to_remove)\n",
    "Eval(output_dir='outputs/', strategy=\"tail\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ea9ea",
   "metadata": {},
   "source": [
    "## ALL ROUGE SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "965caecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-10 tokens</th>\n",
       "      <th>baseline</th>\n",
       "      <th>head_only</th>\n",
       "      <th>tail_only</th>\n",
       "      <th>head+tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>22.2732</td>\n",
       "      <td>20.3532</td>\n",
       "      <td>21.8271</td>\n",
       "      <td>21.9557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.4585</td>\n",
       "      <td>3.3742</td>\n",
       "      <td>4.3329</td>\n",
       "      <td>4.2895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.3347</td>\n",
       "      <td>14.2553</td>\n",
       "      <td>16.0445</td>\n",
       "      <td>16.1649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.3285</td>\n",
       "      <td>14.2423</td>\n",
       "      <td>16.0301</td>\n",
       "      <td>16.1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  -10 tokens  baseline  head_only  tail_only  head+tail\n",
       "0     rouge1   22.2732    20.3532    21.8271    21.9557\n",
       "1     rouge2    4.4585     3.3742     4.3329     4.2895\n",
       "2     rougeL   16.3347    14.2553    16.0445    16.1649\n",
       "3  rougeLsum   16.3285    14.2423    16.0301    16.1509\n",
       "4    gen_len    1.0000     1.0000     1.0000     1.0000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_remove = 10\n",
    "comparison_df_10 = pd.read_csv('outputs/rouge_score_none_0.csv').rename(columns={'0': 'baseline', \n",
    "                                                                                'Unnamed: 0': f'-{tokens_to_remove} tokens'})\n",
    "comparison_df_10['head_only']= pd.read_csv(f'outputs/rouge_score_tail_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_10['tail_only'] = pd.read_csv(f'outputs/rouge_score_head_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_10['head+tail'] = pd.read_csv(f'outputs/rouge_score_middle_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c259fe59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-20 tokens</th>\n",
       "      <th>baseline</th>\n",
       "      <th>head_only</th>\n",
       "      <th>tail_only</th>\n",
       "      <th>head+tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>22.2732</td>\n",
       "      <td>20.2834</td>\n",
       "      <td>21.8455</td>\n",
       "      <td>21.7266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.4585</td>\n",
       "      <td>3.2438</td>\n",
       "      <td>4.3569</td>\n",
       "      <td>4.1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.3347</td>\n",
       "      <td>14.1731</td>\n",
       "      <td>16.1102</td>\n",
       "      <td>15.9697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.3285</td>\n",
       "      <td>14.1632</td>\n",
       "      <td>16.1011</td>\n",
       "      <td>15.9454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  -20 tokens  baseline  head_only  tail_only  head+tail\n",
       "0     rouge1   22.2732    20.2834    21.8455    21.7266\n",
       "1     rouge2    4.4585     3.2438     4.3569     4.1710\n",
       "2     rougeL   16.3347    14.1731    16.1102    15.9697\n",
       "3  rougeLsum   16.3285    14.1632    16.1011    15.9454\n",
       "4    gen_len    1.0000     1.0000     1.0000     1.0000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_remove = 20\n",
    "comparison_df_20 = pd.read_csv('outputs/rouge_score_none_0.csv').rename(columns={'0': 'baseline', \n",
    "                                                                                'Unnamed: 0': f'-{tokens_to_remove} tokens'})\n",
    "comparison_df_20['head_only']= pd.read_csv(f'outputs/rouge_score_tail_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_20['tail_only'] = pd.read_csv(f'outputs/rouge_score_head_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_20['head+tail'] = pd.read_csv(f'outputs/rouge_score_middle_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48815a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-50 tokens</th>\n",
       "      <th>baseline</th>\n",
       "      <th>head_only</th>\n",
       "      <th>tail_only</th>\n",
       "      <th>head+tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>22.2732</td>\n",
       "      <td>19.8081</td>\n",
       "      <td>20.4727</td>\n",
       "      <td>21.0320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.4585</td>\n",
       "      <td>2.9725</td>\n",
       "      <td>3.7730</td>\n",
       "      <td>3.8092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.3347</td>\n",
       "      <td>13.8637</td>\n",
       "      <td>15.0436</td>\n",
       "      <td>15.5697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.3285</td>\n",
       "      <td>13.8608</td>\n",
       "      <td>15.0360</td>\n",
       "      <td>15.5187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  -50 tokens  baseline  head_only  tail_only  head+tail\n",
       "0     rouge1   22.2732    19.8081    20.4727    21.0320\n",
       "1     rouge2    4.4585     2.9725     3.7730     3.8092\n",
       "2     rougeL   16.3347    13.8637    15.0436    15.5697\n",
       "3  rougeLsum   16.3285    13.8608    15.0360    15.5187\n",
       "4    gen_len    1.0000     1.0000     1.0000     1.0000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_remove = 50\n",
    "comparison_df_50 = pd.read_csv('outputs/rouge_score_none_0.csv').rename(columns={'0': 'baseline', \n",
    "                                                                                'Unnamed: 0': f'-{tokens_to_remove} tokens'})\n",
    "comparison_df_50['head_only']= pd.read_csv(f'outputs/rouge_score_tail_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_50['tail_only'] = pd.read_csv(f'outputs/rouge_score_head_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_50['head+tail'] = pd.read_csv(f'outputs/rouge_score_middle_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78d1f2a",
   "metadata": {},
   "source": [
    "## BERT SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e6843a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-10 tokens</th>\n",
       "      <th>none</th>\n",
       "      <th>head_only</th>\n",
       "      <th>tail_only</th>\n",
       "      <th>head+tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.235331</td>\n",
       "      <td>0.157909</td>\n",
       "      <td>0.233151</td>\n",
       "      <td>0.230292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.216859</td>\n",
       "      <td>0.212301</td>\n",
       "      <td>0.207328</td>\n",
       "      <td>0.212871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.226175</td>\n",
       "      <td>0.185327</td>\n",
       "      <td>0.220371</td>\n",
       "      <td>0.221640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  -10 tokens      none  head_only  tail_only  head+tail\n",
       "0  precision  0.235331   0.157909   0.233151   0.230292\n",
       "1     recall  0.216859   0.212301   0.207328   0.212871\n",
       "2         F1  0.226175   0.185327   0.220371   0.221640"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_remove = 10\n",
    "bert_df = pd.read_csv(f'outputs/bert_score_none_0.csv').rename(columns={'0': 'none',\n",
    "\n",
    "                                                                        'Unnamed: 0': f'-{tokens_to_remove} tokens'})\n",
    "bert_df['head_only'] = pd.read_csv(f'outputs/bert_score_tail_{tokens_to_remove}.csv')['0']\n",
    "bert_df['tail_only'] = pd.read_csv(f'outputs/bert_score_head_{tokens_to_remove}.csv')['0']\n",
    "bert_df['head+tail'] = pd.read_csv(f'outputs/bert_score_middle_{tokens_to_remove}.csv')['0']\n",
    "bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03766f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-20 tokens</th>\n",
       "      <th>none</th>\n",
       "      <th>head_only</th>\n",
       "      <th>tail_only</th>\n",
       "      <th>head+tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.235331</td>\n",
       "      <td>0.159912</td>\n",
       "      <td>0.233948</td>\n",
       "      <td>0.223344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.216859</td>\n",
       "      <td>0.213422</td>\n",
       "      <td>0.201985</td>\n",
       "      <td>0.210321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.226175</td>\n",
       "      <td>0.186854</td>\n",
       "      <td>0.218017</td>\n",
       "      <td>0.216909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  -20 tokens      none  head_only  tail_only  head+tail\n",
       "0  precision  0.235331   0.159912   0.233948   0.223344\n",
       "1     recall  0.216859   0.213422   0.201985   0.210321\n",
       "2         F1  0.226175   0.186854   0.218017   0.216909"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_remove = 20\n",
    "bert_df20 = pd.read_csv(f'outputs/bert_score_none_0.csv').rename(columns={'0': 'none',\n",
    "\n",
    "                                                                        'Unnamed: 0': f'-{tokens_to_remove} tokens'})\n",
    "bert_df20['head_only'] = pd.read_csv(f'outputs/bert_score_tail_{tokens_to_remove}.csv')['0']\n",
    "bert_df20['tail_only'] = pd.read_csv(f'outputs/bert_score_head_{tokens_to_remove}.csv')['0']\n",
    "bert_df20['head+tail'] = pd.read_csv(f'outputs/bert_score_middle_{tokens_to_remove}.csv')['0']\n",
    "bert_df20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4aeeb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-50 tokens</th>\n",
       "      <th>none</th>\n",
       "      <th>head_only</th>\n",
       "      <th>tail_only</th>\n",
       "      <th>head+tail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.235331</td>\n",
       "      <td>0.135012</td>\n",
       "      <td>0.213281</td>\n",
       "      <td>0.210393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.216859</td>\n",
       "      <td>0.203137</td>\n",
       "      <td>0.170915</td>\n",
       "      <td>0.204667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.226175</td>\n",
       "      <td>0.168881</td>\n",
       "      <td>0.191823</td>\n",
       "      <td>0.207497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  -50 tokens      none  head_only  tail_only  head+tail\n",
       "0  precision  0.235331   0.135012   0.213281   0.210393\n",
       "1     recall  0.216859   0.203137   0.170915   0.204667\n",
       "2         F1  0.226175   0.168881   0.191823   0.207497"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_remove = 50\n",
    "bert_df50 = pd.read_csv(f'outputs/bert_score_none_0.csv').rename(columns={'0': 'none',\n",
    "\n",
    "                                                                        'Unnamed: 0': f'-{tokens_to_remove} tokens'})\n",
    "bert_df50['head_only'] = pd.read_csv(f'outputs/bert_score_tail_{tokens_to_remove}.csv')['0']\n",
    "bert_df50['tail_only'] = pd.read_csv(f'outputs/bert_score_head_{tokens_to_remove}.csv')['0']\n",
    "bert_df50['head+tail'] = pd.read_csv(f'outputs/bert_score_middle_{tokens_to_remove}.csv')['0']\n",
    "bert_df50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941963b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
