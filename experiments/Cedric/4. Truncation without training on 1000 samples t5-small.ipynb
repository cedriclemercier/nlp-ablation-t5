{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f49b3c",
   "metadata": {},
   "source": [
    "# 4. Truncation without training on 1000 samples t5-small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219af75",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c0215b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% | 23% |\n",
      "|  1 |  0% |  0% |\n",
      "|  2 |  0% |  0% |\n",
      "|  3 |  0% |  0% |\n"
     ]
    }
   ],
   "source": [
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939b01c",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef95dd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "\n",
    "from bert_score import score\n",
    "from bert_score import plot_example\n",
    "\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, tokenization_utils_base, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "device = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0aa9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "with open('../../datasets/test_set.txt') as json_file:\n",
    "    test_set = json.load(json_file)\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb95ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P, R, F1 = score([test_set['document'][0], test_set['document'][1]], [test_set['summary'][0], test_set['summary'][1]], lang='en', rescale_with_baseline=True, device=device)\n",
    "# # P, R, F1 = scorer.score([test_set['document'][0]], [test_set['summary'][0]])\n",
    "# # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "# print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6659e9b",
   "metadata": {},
   "source": [
    "## 2. Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b953373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, actuals, tokenizer):\n",
    "    \n",
    "    # <your code here>\n",
    "    metric = load_metric(\"rouge\")\n",
    "    result = metric.compute(predictions=predictions, references=actuals, use_stemmer=True)\n",
    "    \n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    rouge = {k: round(v, 4) for k, v in result.items()}\n",
    "    \n",
    "    P, R, F1 = score(predictions, actuals, lang='en', rescale_with_baseline=True, device=device)\n",
    "    \n",
    "    bert = {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'F1': F1.mean().item(),\n",
    "    }\n",
    "\n",
    "    return rouge, bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44abbe",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd42387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define model parameters specific to BART\n",
    "model_params = {\n",
    "    \"MODEL\": \"gniemiec/t5-small-finetuned-xsum\",\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 36,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a144c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded gniemiec/t5-small-finetuned-xsum\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "print(f\"Loaded {model_params['MODEL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d33621",
   "metadata": {},
   "source": [
    "## 4. Test 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "534ddc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Police have recovered three firearms, ammunition and a five-figure sum of money.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # tokens = tokenizer(test_set['document'][1], return_tensors=\"pt\")\n",
    "    tokens = tokenizer.batch_encode_plus([test_set['document'][0]], \n",
    "                                         max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"], \n",
    "                                         truncation=True, \n",
    "                                         padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    # outputs = model.generate(tokens)\n",
    "    outputs = model.to(device).generate(\n",
    "                      input_ids = tokens.input_ids,\n",
    "                      attention_mask = tokens.attention_mask, \n",
    "                      max_length=150, \n",
    "                      num_beams=2,\n",
    "                      repetition_penalty=2.5, \n",
    "                      length_penalty=1.0, \n",
    "                      early_stopping=True\n",
    "                      )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428f0626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge1</th>\n",
       "      <td>33.3333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2</th>\n",
       "      <td>14.2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL</th>\n",
       "      <td>20.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum</th>\n",
       "      <td>20.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gen_len</th>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "rouge1     33.3333\n",
       "rouge2     14.2857\n",
       "rougeL     20.0000\n",
       "rougeLsum  20.0000\n",
       "gen_len     1.0000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge, bert = compute_metrics([decoded], [test_set['summary'][0]], tokenizer)   \n",
    "rouge_df = pd.DataFrame.from_dict(rouge, orient='index')\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aadac57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.499494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.473107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.487103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "precision  0.499494\n",
       "recall     0.473107\n",
       "F1         0.487103"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df = pd.DataFrame.from_dict(bert, orient='index')\n",
    "bert_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a471d52",
   "metadata": {},
   "source": [
    "## 5. Evaluate on 1000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af7c97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix} | {iteration}/{total}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b42def54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# model = model.to(device)\n",
    "# predictions = []\n",
    "\n",
    "# l = len(test_set['document'])\n",
    "# printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "# with torch.no_grad():\n",
    "#     for idx, doc in enumerate(test_set['document']):\n",
    "#         printProgressBar(idx + 1, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "#         tokens = tokenizer.batch_encode_plus([doc], \n",
    "#                                              max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"], \n",
    "#                                              truncation=True, \n",
    "#                                              padding=\"max_length\", \n",
    "#                                              return_tensors=\"pt\").to(device)\n",
    "       \n",
    "#         outputs = model.generate(\n",
    "#                       input_ids = tokens.input_ids,\n",
    "#                       attention_mask = tokens.attention_mask, \n",
    "#                       max_length=150, \n",
    "#                       num_beams=2,\n",
    "#                       repetition_penalty=2.5, \n",
    "#                       length_penalty=1.0, \n",
    "#                       early_stopping=True\n",
    "#                       )\n",
    "#         decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#         predictions.append(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d22faa3",
   "metadata": {},
   "source": [
    "### 5.1 Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20339dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(predictions, actuals, output):\n",
    "    df = pd.DataFrame({'predictions': predictions, 'actuals': actuals})\n",
    "    df.to_csv(output)\n",
    "    print(\"PREDICTIONS RESULTS SAVED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e64865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_results(predictions, test_set['summary'], 'outputs/predictions_pretrained_t5xsum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c133c37",
   "metadata": {},
   "source": [
    "## 6. Create truncated tokens list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7011cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Eval(output_dir, strategy=\"none\", no_tokens=0, from_left = 0.2):\n",
    "    '''\n",
    "    source: array of documents\n",
    "    strategy:\n",
    "        - head: truncate head\n",
    "        - tail: truncate tail \n",
    "        - both: truncate head and tail\n",
    "        - middle: truncate middle words\n",
    "    words_no: number of tokens to remove\n",
    "    '''\n",
    "    print(f\"Strategy: Remove {strategy} tokens\")\n",
    "    l = len(test_set['document'])\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seedtokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    \n",
    "    print(\"Loading model..\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_params[\"MODEL\"])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    printProgressBar(0, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, doc in enumerate(test_set['document']):\n",
    "            printProgressBar(idx + 1, l, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "            tokens = tokenizer.batch_encode_plus([doc], \n",
    "                                                 max_length=model_params[\"MAX_SOURCE_TEXT_LENGTH\"], \n",
    "                                                 truncation=True, \n",
    "                                                 padding=\"max_length\", \n",
    "                                                 return_tensors=\"pt\").to(device)\n",
    "            \n",
    "            source = tokens.input_ids[0]\n",
    "            attention = tokens.attention_mask[0]\n",
    "            \n",
    "            if strategy == \"middle\":\n",
    "                \n",
    "                left_remove_percent = from_left\n",
    "                \n",
    "                no_real_tokens = sum(x != 0 for x in source)\n",
    "                left = int(left_remove_percent * no_real_tokens)\n",
    "                \n",
    "                # 3. Truncate left and right, concat both (input ids and attention mask)\n",
    "                left_selection_ids = source[:left]\n",
    "                right_selection_ids = source[left+no_tokens:len(source)]\n",
    "\n",
    "                left_attention_mask = attention[:left]\n",
    "                right_attention_mask = attention[left+no_tokens:len(source)]\n",
    "\n",
    "                new_ids = torch.concat([left_selection_ids, right_selection_ids], 0)        \n",
    "                new_masks = torch.concat([left_attention_mask,right_attention_mask], 0)\n",
    "\n",
    "                new_ids = new_ids.tolist()\n",
    "                new_masks = new_masks.tolist()\n",
    "\n",
    "                # 4. Create a new source text\n",
    "                new_encoding = {\n",
    "                    'input_ids': torch.IntTensor([new_ids]).to(dtype=torch.long),\n",
    "                    'attention_mask': torch.IntTensor([new_masks]).to(dtype=torch.long)\n",
    "                }\n",
    "                \n",
    "\n",
    "                # 5. Set new token encoding\n",
    "                new_tokens = tokenization_utils_base.BatchEncoding(new_encoding).to(device)\n",
    "                \n",
    "                \n",
    "            elif strategy == \"head\":\n",
    "                new_ids = source[no_tokens:].tolist()\n",
    "                new_masks = attention[no_tokens:].tolist()\n",
    "\n",
    "                # 4. Create a new source text\n",
    "                new_encoding = {\n",
    "                    'input_ids': torch.IntTensor([new_ids]).to(dtype=torch.long),\n",
    "                    'attention_mask': torch.IntTensor([new_masks]).to(dtype=torch.long)\n",
    "                }\n",
    "                \n",
    "\n",
    "                # 5. Set new token encoding\n",
    "                new_tokens = tokenization_utils_base.BatchEncoding(new_encoding).to(device)\n",
    "                \n",
    "            elif strategy == \"tail\":\n",
    "                no_real_tokens = sum(x != 0 for x in source)\n",
    "                zeros = source[no_real_tokens:]\n",
    "                \n",
    "                new_ids = torch.concat([source[:no_real_tokens-no_tokens],zeros], 0).tolist()\n",
    "                new_masks = torch.concat([attention[:no_real_tokens-no_tokens],zeros], 0).tolist()\n",
    "                \n",
    "                \n",
    "                new_encoding = {\n",
    "                    'input_ids': torch.IntTensor([new_ids]).to(dtype=torch.long),\n",
    "                    'attention_mask': torch.IntTensor([new_masks]).to(dtype=torch.long)\n",
    "                }\n",
    "                new_tokens = tokenization_utils_base.BatchEncoding(new_encoding).to(device)\n",
    "                \n",
    "            elif strategy == \"both\":\n",
    "                pass\n",
    "            else:\n",
    "                new_tokens = tokens\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                          input_ids = new_tokens.input_ids,\n",
    "                          attention_mask = new_tokens.attention_mask, \n",
    "                          max_length=150, \n",
    "                          num_beams=2,\n",
    "                          repetition_penalty=2.5, \n",
    "                          length_penalty=1.0, \n",
    "                          early_stopping=True\n",
    "                          )\n",
    "            \n",
    "            decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predictions.append(decoded)\n",
    "            \n",
    "    save_results(predictions, test_set['summary'], os.path.join(output_dir,f'predictions_{strategy}_{no_tokens}.csv'))\n",
    "    \n",
    "    rouge, bert = compute_metrics(predictions, test_set['summary'], tokenizer)\n",
    "    \n",
    "    rouge_df = pd.DataFrame.from_dict(rouge, orient='index')\n",
    "    rouge_df.to_csv(os.path.join(output_dir, f'rouge_score_{strategy}_{no_tokens}.csv'))\n",
    "    \n",
    "    bert_df = pd.DataFrame.from_dict(bert, orient='index')\n",
    "    bert_df.to_csv(os.path.join(output_dir, f'bert_score_{strategy}_{no_tokens}.csv'))\n",
    "    \n",
    "    print(f\"SAVE ROUGE TO CSV FINISHED @ {os.path.join(output_dir, f'rouge_score_{strategy}_{no_tokens}.csv')}\")\n",
    "    print(f\"SAVE BERT-SCORE TO CSV FINISHED @ {os.path.join(output_dir, f'bert_score_{strategy}_{no_tokens}.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3b5e3",
   "metadata": {},
   "source": [
    "## 7. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb8b18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_remove = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac61102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove middle tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_middle_20.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_middle_20.csv\n"
     ]
    }
   ],
   "source": [
    "# #middle words\n",
    "Eval(output_dir='outputs/', strategy=\"middle\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58d0d214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>21.7266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>15.9697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>15.9454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0        0\n",
       "0     rouge1  21.7266\n",
       "1     rouge2   4.1710\n",
       "2     rougeL  15.9697\n",
       "3  rougeLsum  15.9454\n",
       "4    gen_len   1.0000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df_middle = pd.read_csv(f'outputs/rouge_score_middle_{tokens_to_remove}.csv')\n",
    "rouge_df_middle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdc781d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove none tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_none_0.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_none_0.csv\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "Eval(output_dir='outputs/', strategy=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91d01335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>22.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.4585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.3347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.3285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0        0\n",
       "0     rouge1  22.2732\n",
       "1     rouge2   4.4585\n",
       "2     rougeL  16.3347\n",
       "3  rougeLsum  16.3285\n",
       "4    gen_len   1.0000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df = pd.read_csv('outputs/rouge_score_none_0.csv')\n",
    "rouge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59eb547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove head tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_head_20.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_head_20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "#head words\n",
    "Eval(output_dir='outputs/', strategy=\"head\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88756085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>21.8455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.3569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.1011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0        0\n",
       "0     rouge1  21.8455\n",
       "1     rouge2   4.3569\n",
       "2     rougeL  16.1102\n",
       "3  rougeLsum  16.1011\n",
       "4    gen_len   1.0000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df_head = pd.read_csv(f'outputs/rouge_score_head_{tokens_to_remove}.csv')\n",
    "rouge_df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7430686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove tail tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_tail_20.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_tail_20.csv\n"
     ]
    }
   ],
   "source": [
    "#tail words\n",
    "Eval(output_dir='outputs/', strategy=\"tail\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "753d07a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>20.2834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>3.2438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>14.1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>14.1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0        0\n",
       "0     rouge1  20.2834\n",
       "1     rouge2   3.2438\n",
       "2     rougeL  14.1731\n",
       "3  rougeLsum  14.1632\n",
       "4    gen_len   1.0000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_df_tail = pd.read_csv(f'outputs/rouge_score_tail_{tokens_to_remove}.csv')\n",
    "rouge_df_tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4bb926",
   "metadata": {},
   "source": [
    "## 8. RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd05af",
   "metadata": {},
   "source": [
    "### ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41068834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>none</th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "      <th>middle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>22.2732</td>\n",
       "      <td>21.8455</td>\n",
       "      <td>20.2834</td>\n",
       "      <td>21.7266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.4585</td>\n",
       "      <td>4.3569</td>\n",
       "      <td>3.2438</td>\n",
       "      <td>4.1710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.3347</td>\n",
       "      <td>16.1102</td>\n",
       "      <td>14.1731</td>\n",
       "      <td>15.9697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.3285</td>\n",
       "      <td>16.1011</td>\n",
       "      <td>14.1632</td>\n",
       "      <td>15.9454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0     none     head     tail   middle\n",
       "0     rouge1  22.2732  21.8455  20.2834  21.7266\n",
       "1     rouge2   4.4585   4.3569   3.2438   4.1710\n",
       "2     rougeL  16.3347  16.1102  14.1731  15.9697\n",
       "3  rougeLsum  16.3285  16.1011  14.1632  15.9454\n",
       "4    gen_len   1.0000   1.0000   1.0000   1.0000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = rouge_df.rename(columns={'0': 'none'})\n",
    "comparison_df['head'] =rouge_df_head['0']\n",
    "comparison_df['tail']= rouge_df_tail['0']\n",
    "comparison_df['middle'] = rouge_df_middle['0']\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027d894",
   "metadata": {},
   "source": [
    "### BERTSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ac080ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>none</th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "      <th>middle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.235331</td>\n",
       "      <td>0.233948</td>\n",
       "      <td>0.159912</td>\n",
       "      <td>0.223344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.216859</td>\n",
       "      <td>0.201985</td>\n",
       "      <td>0.213422</td>\n",
       "      <td>0.210321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.226175</td>\n",
       "      <td>0.218017</td>\n",
       "      <td>0.186854</td>\n",
       "      <td>0.216909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0      none      head      tail    middle\n",
       "0  precision  0.235331  0.233948  0.159912  0.223344\n",
       "1     recall  0.216859  0.201985  0.213422  0.210321\n",
       "2         F1  0.226175  0.218017  0.186854  0.216909"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df = pd.read_csv(f'outputs/bert_score_none_0.csv').rename(columns={'0': 'none'})\n",
    "bert_df['head'] = pd.read_csv(f'outputs/bert_score_head_{tokens_to_remove}.csv')['0']\n",
    "bert_df['tail'] = pd.read_csv(f'outputs/bert_score_tail_{tokens_to_remove}.csv')['0']\n",
    "bert_df['middle'] = pd.read_csv(f'outputs/bert_score_middle_{tokens_to_remove}.csv')['0']\n",
    "bert_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a2866",
   "metadata": {},
   "source": [
    "## Trying with 50 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dbcb149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove middle tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_middle_50.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_middle_50.csv\n",
      "Strategy: Remove head tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_head_50.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_head_50.csv\n",
      "Strategy: Remove tail tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_tail_50.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_tail_50.csv\n"
     ]
    }
   ],
   "source": [
    "tokens_to_remove = 50\n",
    "Eval(output_dir='outputs/', strategy=\"middle\", no_tokens=tokens_to_remove)\n",
    "Eval(output_dir='outputs/', strategy=\"head\", no_tokens=tokens_to_remove)\n",
    "Eval(output_dir='outputs/', strategy=\"tail\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34d54ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>none</th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "      <th>middle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>22.2732</td>\n",
       "      <td>20.4727</td>\n",
       "      <td>19.8081</td>\n",
       "      <td>21.0320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.4585</td>\n",
       "      <td>3.7730</td>\n",
       "      <td>2.9725</td>\n",
       "      <td>3.8092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.3347</td>\n",
       "      <td>15.0436</td>\n",
       "      <td>13.8637</td>\n",
       "      <td>15.5697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.3285</td>\n",
       "      <td>15.0360</td>\n",
       "      <td>13.8608</td>\n",
       "      <td>15.5187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0     none     head     tail   middle\n",
       "0     rouge1  22.2732  20.4727  19.8081  21.0320\n",
       "1     rouge2   4.4585   3.7730   2.9725   3.8092\n",
       "2     rougeL  16.3347  15.0436  13.8637  15.5697\n",
       "3  rougeLsum  16.3285  15.0360  13.8608  15.5187\n",
       "4    gen_len   1.0000   1.0000   1.0000   1.0000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df_50 = pd.read_csv('outputs/rouge_score_none_0.csv').rename(columns={'0': 'none'})\n",
    "comparison_df_50['head'] = pd.read_csv(f'outputs/rouge_score_head_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_50['tail']= pd.read_csv(f'outputs/rouge_score_tail_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_50['middle'] = pd.read_csv(f'outputs/rouge_score_middle_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44827817",
   "metadata": {},
   "source": [
    "## Trying with 10 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f712dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy: Remove middle tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_middle_10.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_middle_10.csv\n",
      "Strategy: Remove head tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_head_10.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_head_10.csv\n",
      "Strategy: Remove tail tokens\n",
      "Loading model..\n",
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete | 1000/1000\n",
      "PREDICTIONS RESULTS SAVED.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE ROUGE TO CSV FINISHED @ outputs/rouge_score_tail_10.csv\n",
      "SAVE BERT-SCORE TO CSV FINISHED @ outputs/bert_score_tail_10.csv\n"
     ]
    }
   ],
   "source": [
    "tokens_to_remove = 10\n",
    "Eval(output_dir='outputs/', strategy=\"middle\", no_tokens=tokens_to_remove)\n",
    "Eval(output_dir='outputs/', strategy=\"head\", no_tokens=tokens_to_remove)\n",
    "Eval(output_dir='outputs/', strategy=\"tail\", no_tokens=tokens_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "965caecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>none</th>\n",
       "      <th>head</th>\n",
       "      <th>tail</th>\n",
       "      <th>middle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1</td>\n",
       "      <td>22.2732</td>\n",
       "      <td>21.8271</td>\n",
       "      <td>20.3532</td>\n",
       "      <td>21.9557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge2</td>\n",
       "      <td>4.4585</td>\n",
       "      <td>4.3329</td>\n",
       "      <td>3.3742</td>\n",
       "      <td>4.2895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rougeL</td>\n",
       "      <td>16.3347</td>\n",
       "      <td>16.0445</td>\n",
       "      <td>14.2553</td>\n",
       "      <td>16.1649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rougeLsum</td>\n",
       "      <td>16.3285</td>\n",
       "      <td>16.0301</td>\n",
       "      <td>14.2423</td>\n",
       "      <td>16.1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gen_len</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0     none     head     tail   middle\n",
       "0     rouge1  22.2732  21.8271  20.3532  21.9557\n",
       "1     rouge2   4.4585   4.3329   3.3742   4.2895\n",
       "2     rougeL  16.3347  16.0445  14.2553  16.1649\n",
       "3  rougeLsum  16.3285  16.0301  14.2423  16.1509\n",
       "4    gen_len   1.0000   1.0000   1.0000   1.0000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df_10 = pd.read_csv('outputs/rouge_score_none_0.csv').rename(columns={'0': 'none'})\n",
    "comparison_df_10['head'] = pd.read_csv(f'outputs/rouge_score_head_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_10['tail']= pd.read_csv(f'outputs/rouge_score_tail_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_10['middle'] = pd.read_csv(f'outputs/rouge_score_middle_{tokens_to_remove}.csv')['0']\n",
    "comparison_df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b3ccc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8675b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
